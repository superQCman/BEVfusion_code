// This file is computer-generated by onnx2c 
// (TODO: add creating command line here)
// (TODO: print creation date here )

// ONNX model:
// produced by pytorch, version 1.10
// ONNX IR version: 14
// Model documentation: 
/*

*/

#include <float.h>
#include <math.h>
#include <stdbool.h>
#include <stdint.h>
#include <string.h>
#include <half.hpp>
#include "readTensorFromFile.h"
#define MAX(X,Y) ( X > Y ? X : Y)
#define MIN(X,Y) ( X < Y ? X : Y)
#define CLIP(X,L) ( MAX(MIN(X,L), -L) )

/*
 * Operand:           Concat
 * Name in ONNX file: Concat_0
 */
static inline void node_Concat_0( const float input_0[1][80][180][180], const float input_1[1][256][180][180], float output[1][336][180][180] )
{
	/* Concat */
	int64_t outputOffset;
	outputOffset = 0;
	for (int64_t i = 0, j = 0; i < 2592000; i++) {
		*((float*)output + (outputOffset + i)) = *((float*)input_0 + i);
		if (++j == 2592000) {
			outputOffset += (8294400);
			j = 0;
		}
	}
	outputOffset = 2592000;
	for (int64_t i = 0, j = 0; i < 8294400; i++) {
		*((float*)output + (outputOffset + i)) = *((float*)input_1 + i);
		if (++j == 8294400) {
			outputOffset += (2592000);
			j = 0;
		}
	}
}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_1
 */
static inline void node_Conv_1( const float x[1][336][180][180], const float w[256][336][3][3], const float bias[256], float y[1][256][180][180] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<256; m++) {
		for( int32_t o0=0, i0=-1; o0<180; o0++, i0+=1) {
		for( int32_t o1=0, i1=-1; o1<180; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<336; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=180) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=180) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_2
 */
static inline void node_Relu_2( const float X[1][256][180][180], float Y[1][256][180][180] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<8294400; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_3
 */
static inline void node_Conv_3( const float x[1][256][180][180], const float w[128][256][3][3], const float bias[128], float y[1][128][180][180] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<128; m++) {
		for( int32_t o0=0, i0=-1; o0<180; o0++, i0+=1) {
		for( int32_t o1=0, i1=-1; o1<180; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<256; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=180) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=180) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_4
 */
static inline void node_Relu_4( const float X[1][128][180][180], float Y[1][128][180][180] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<4147200; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_5
 */
static inline void node_Conv_5( const float x[1][128][180][180], const float w[128][128][3][3], const float bias[128], float y[1][128][180][180] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<128; m++) {
		for( int32_t o0=0, i0=-1; o0<180; o0++, i0+=1) {
		for( int32_t o1=0, i1=-1; o1<180; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<128; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=180) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=180) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_6
 */
static inline void node_Relu_6( const float X[1][128][180][180], float Y[1][128][180][180] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<4147200; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_7
 */
static inline void node_Conv_7( const float x[1][128][180][180], const float w[128][128][3][3], const float bias[128], float y[1][128][180][180] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<128; m++) {
		for( int32_t o0=0, i0=-1; o0<180; o0++, i0+=1) {
		for( int32_t o1=0, i1=-1; o1<180; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<128; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=180) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=180) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_8
 */
static inline void node_Relu_8( const float X[1][128][180][180], float Y[1][128][180][180] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<4147200; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_9
 */
static inline void node_Conv_9( const float x[1][128][180][180], const float w[128][128][3][3], const float bias[128], float y[1][128][180][180] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<128; m++) {
		for( int32_t o0=0, i0=-1; o0<180; o0++, i0+=1) {
		for( int32_t o1=0, i1=-1; o1<180; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<128; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=180) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=180) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_10
 */
static inline void node_Relu_10( const float X[1][128][180][180], float Y[1][128][180][180] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<4147200; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_11
 */
static inline void node_Conv_11( const float x[1][128][180][180], const float w[128][128][3][3], const float bias[128], float y[1][128][180][180] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<128; m++) {
		for( int32_t o0=0, i0=-1; o0<180; o0++, i0+=1) {
		for( int32_t o1=0, i1=-1; o1<180; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<128; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=180) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=180) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_12
 */
static inline void node_Relu_12( const float X[1][128][180][180], float Y[1][128][180][180] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<4147200; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_13
 */
static inline void node_Conv_13( const float x[1][128][180][180], const float w[128][128][3][3], const float bias[128], float y[1][128][180][180] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<128; m++) {
		for( int32_t o0=0, i0=-1; o0<180; o0++, i0+=1) {
		for( int32_t o1=0, i1=-1; o1<180; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<128; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=180) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=180) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_14
 */
static inline void node_Relu_14( const float X[1][128][180][180], float Y[1][128][180][180] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<4147200; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_15
 */
static inline void node_Conv_15( const float x[1][128][180][180], const float w[256][128][3][3], const float bias[256], float y[1][256][90][90] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 2 2 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<256; m++) {
		for( int32_t o0=0, i0=-1; o0<90; o0++, i0+=2) {
		for( int32_t o1=0, i1=-1; o1<90; o1++, i1+=2) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<128; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=180) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=180) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_16
 */
static inline void node_Relu_16( const float X[1][256][90][90], float Y[1][256][90][90] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<2073600; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_17
 */
static inline void node_Conv_17( const float x[1][256][90][90], const float w[256][256][3][3], const float bias[256], float y[1][256][90][90] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<256; m++) {
		for( int32_t o0=0, i0=-1; o0<90; o0++, i0+=1) {
		for( int32_t o1=0, i1=-1; o1<90; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<256; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=90) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=90) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_18
 */
static inline void node_Relu_18( const float X[1][256][90][90], float Y[1][256][90][90] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<2073600; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_19
 */
static inline void node_Conv_19( const float x[1][256][90][90], const float w[256][256][3][3], const float bias[256], float y[1][256][90][90] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<256; m++) {
		for( int32_t o0=0, i0=-1; o0<90; o0++, i0+=1) {
		for( int32_t o1=0, i1=-1; o1<90; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<256; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=90) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=90) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_20
 */
static inline void node_Relu_20( const float X[1][256][90][90], float Y[1][256][90][90] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<2073600; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_21
 */
static inline void node_Conv_21( const float x[1][256][90][90], const float w[256][256][3][3], const float bias[256], float y[1][256][90][90] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<256; m++) {
		for( int32_t o0=0, i0=-1; o0<90; o0++, i0+=1) {
		for( int32_t o1=0, i1=-1; o1<90; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<256; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=90) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=90) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_22
 */
static inline void node_Relu_22( const float X[1][256][90][90], float Y[1][256][90][90] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<2073600; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_23
 */
static inline void node_Conv_23( const float x[1][256][90][90], const float w[256][256][3][3], const float bias[256], float y[1][256][90][90] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<256; m++) {
		for( int32_t o0=0, i0=-1; o0<90; o0++, i0+=1) {
		for( int32_t o1=0, i1=-1; o1<90; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<256; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=90) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=90) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_24
 */
static inline void node_Relu_24( const float X[1][256][90][90], float Y[1][256][90][90] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<2073600; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_25
 */
static inline void node_Conv_25( const float x[1][256][90][90], const float w[256][256][3][3], const float bias[256], float y[1][256][90][90] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 3 3 
	 * pads: 1 1 1 1 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<256; m++) {
		for( int32_t o0=0, i0=-1; o0<90; o0++, i0+=1) {
		for( int32_t o1=0, i1=-1; o1<90; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<256; c++ ) {
			for( uint32_t k0=0; k0<3; k0++ ) {
			for( uint32_t k1=0; k1<3; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=90) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=90) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_26
 */
static inline void node_Relu_26( const float X[1][256][90][90], float Y[1][256][90][90] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<2073600; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Cast
 * Name in ONNX file: Cast_27
 */
static inline void node_Cast_27( const float input[1][128][180][180], float output[1][128][180][180] )
{
	/* Cast */
	float *X = (float*)input;
	float *Y = (float*)output;
	for( unsigned i=0; i<4147200; i++)
		Y[i]= (float)X[i];
}

/*
 * Operand:           Conv
 * Name in ONNX file: Conv_29
 */
static inline void node_Conv_29( const float x[1][128][180][180], const float w[256][128][1][1], const float bias[256], float y[1][256][180][180] )
{
	/* Conv
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 1 1 
	 * pads: 0 0 0 0 
	 * strides: 1 1 
	 */
	for( uint32_t b=0; b<1; b++ ) {
	for( uint32_t m=0; m<256; m++) {
		for( int32_t o0=0, i0=0; o0<180; o0++, i0+=1) {
		for( int32_t o1=0, i1=0; o1<180; o1++, i1+=1) {
			y[b][m][o0][o1] = bias[m];
			for( int32_t c=0; c<128; c++ ) {
			for( uint32_t k0=0; k0<1; k0++ ) {
			for( uint32_t k1=0; k1<1; k1++ ) {
				int ii0 = i0+k0 * 1;
				if( ii0<0) continue;
				if( ii0>=180) continue;
				int ii1 = i1+k1 * 1;
				if( ii1<0) continue;
				if( ii1>=180) continue;
				y[b][m][o0][o1] += x[b][c][ii0][ii1] *w[m][c][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_30
 */
static inline void node_Relu_30( const float X[1][256][180][180], float Y[1][256][180][180] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<8294400; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           ConvTranspose
 * Name in ONNX file: ConvTranspose_31
 */
static inline void node_ConvTranspose_31( const float x[1][256][90][90], const float w[256][256][2][2], float y[1][256][180][180] )
{
	/* ConvTranspose
	 *
	 * auto_pad: NOTSET
	 * dilations: 1 1 
	 * group: 1
	 * kernel_shape: 2 2 
	 * pads: 0 0 0 0 
	 * strides: 2 2 
	 * output_padding: 0 0 
	 * output_shape: 180 180 
	 * output_shape explicitly given in ONNX model: false
	 */
	memset(y, 0,16588800);

	for( uint32_t b=0; b<125627568; b++ ) {
	for( uint32_t m=0; m<256; m++) {
		for( int32_t i0=0; i0<-1776299056; i0++) {
		for( int32_t i1=0; i1<22023; i1++) {
			for( int32_t c=0; c<32682; c++ ) {
			for( int32_t k0=0, o0=i0*2-0; k0<2; k0++, o0+=1) {
			for( int32_t k1=0, o1=i1*2-0; k1<2; k1++, o1+=1) {
				if( o0<0) continue;
				if( o0>=180) continue;
				if( o1<0) continue;
				if( o1>=180) continue;
				y[b][m][o0][o1] += x[b][c][i0][i1] * w[c][m][k0][k1];
			} /* k */
			} /* k */
			} /* c */
		} /* o */
		} /* o */
	} /* m */
	} /* b */
}

/*
 * Operand:           BatchNormalization
 * Name in ONNX file: BatchNormalization_32
 */
static inline void node_BatchNormalization_32( const float X[1][256][180][180], const float scale[256], const float bias[256], const float mean[256], const float var[256], float output[1][256][180][180] )
{
	/* BatchNormalization
	 * epsilon = 0.0010000000474974513054
	 * momentum = 0.99000000953674316406
	 */

	for( int32_t b=0; b<1; b++ ) {
	for( int32_t c=0; c<256; c++ ) {
	for( uint32_t i2=0; i2<180; i2++ ) {
	for( uint32_t i3=0; i3<180; i3++ ) {
		float tmp_X = ( X[b][c][i2][i3] - mean[c] ) / ( var[c] );
		output[b][c][i2][i3] = tmp_X * scale[c] + bias[c];
	}
	}
	}
	}
}

/*
 * Operand:           Relu
 * Name in ONNX file: Relu_33
 */
static inline void node_Relu_33( const float X[1][256][180][180], float Y[1][256][180][180] )
{
	/*Relu*/
	float *X_ptr = (float*)X;
	float *Y_ptr = (float*)Y;
	uint32_t i;
	for( i=0; i<8294400; i++ )
		Y_ptr[i] = X_ptr[i] > 0 ? X_ptr[i] : 0;

}

/*
 * Operand:           Concat
 * Name in ONNX file: Concat_34
 */
static inline void node_Concat_34( const float input_0[1][256][180][180], const float input_1[1][256][180][180], float output[1][512][180][180] )
{
	/* Concat */
	int64_t outputOffset;
	outputOffset = 0;
	for (int64_t i = 0, j = 0; i < 8294400; i++) {
		*((float*)output + (outputOffset + i)) = *((float*)input_0 + i);
		if (++j == 8294400) {
			outputOffset += (8294400);
			j = 0;
		}
	}
	outputOffset = 8294400;
	for (int64_t i = 0, j = 0; i < 8294400; i++) {
		*((float*)output + (outputOffset + i)) = *((float*)input_1 + i);
		if (++j == 8294400) {
			outputOffset += (8294400);
			j = 0;
		}
	}
}


void entry(float tensor_camera[1][80][180][180], float tensor_lidar[1][256][180][180], float tensor_middle[1][512][180][180]){
	node_Concat_0( tensor_camera, tensor_lidar, tu0.tensor_471);
	node_Conv_1( tu0.tensor_471, tensor_parent_fuser_0_weight, tensor_parent_fuser_0_bias, tu1.tensor_472);
	node_Relu_2( tu1.tensor_472, tu0.tensor_473);
	node_Conv_3( tu0.tensor_473, tensor_parent_decoder_backbone_blocks_0_0_weight, tensor_parent_decoder_backbone_blocks_0_0_bias, tu1.tensor_474);
	node_Relu_4( tu1.tensor_474, tu0.tensor_475);
	node_Conv_5( tu0.tensor_475, tensor_parent_decoder_backbone_blocks_0_3_weight, tensor_parent_decoder_backbone_blocks_0_3_bias, tu1.tensor_476);
	node_Relu_6( tu1.tensor_476, tu0.tensor_477);
	node_Conv_7( tu0.tensor_477, tensor_parent_decoder_backbone_blocks_0_6_weight, tensor_parent_decoder_backbone_blocks_0_6_bias, tu1.tensor_478);
	node_Relu_8( tu1.tensor_478, tu0.tensor_479);
	node_Conv_9( tu0.tensor_479, tensor_parent_decoder_backbone_blocks_0_9_weight, tensor_parent_decoder_backbone_blocks_0_9_bias, tu1.tensor_480);
	node_Relu_10( tu1.tensor_480, tu0.tensor_481);
	node_Conv_11( tu0.tensor_481, tensor_parent_decoder_backbone_blocks_0_12_weight, tensor_parent_decoder_backbone_blocks_0_12_bias, tu1.tensor_482);
	node_Relu_12( tu1.tensor_482, tu0.tensor_483);
	node_Conv_13( tu0.tensor_483, tensor_parent_decoder_backbone_blocks_0_15_weight, tensor_parent_decoder_backbone_blocks_0_15_bias, tu1.tensor_484);
	node_Relu_14( tu1.tensor_484, tu0.tensor_485);
	node_Conv_15( tu0.tensor_485, tensor_parent_decoder_backbone_blocks_1_0_weight, tensor_parent_decoder_backbone_blocks_1_0_bias, tu1.tensor_486);
	node_Relu_16( tu1.tensor_486, tu2.tensor_487);
	node_Conv_17( tu2.tensor_487, tensor_parent_decoder_backbone_blocks_1_3_weight, tensor_parent_decoder_backbone_blocks_1_3_bias, tu1.tensor_488);
	node_Relu_18( tu1.tensor_488, tu2.tensor_489);
	node_Conv_19( tu2.tensor_489, tensor_parent_decoder_backbone_blocks_1_6_weight, tensor_parent_decoder_backbone_blocks_1_6_bias, tu1.tensor_490);
	node_Relu_20( tu1.tensor_490, tu2.tensor_491);
	node_Conv_21( tu2.tensor_491, tensor_parent_decoder_backbone_blocks_1_9_weight, tensor_parent_decoder_backbone_blocks_1_9_bias, tu1.tensor_492);
	node_Relu_22( tu1.tensor_492, tu2.tensor_493);
	node_Conv_23( tu2.tensor_493, tensor_parent_decoder_backbone_blocks_1_12_weight, tensor_parent_decoder_backbone_blocks_1_12_bias, tu1.tensor_494);
	node_Relu_24( tu1.tensor_494, tu2.tensor_495);
	node_Conv_25( tu2.tensor_495, tensor_parent_decoder_backbone_blocks_1_15_weight, tensor_parent_decoder_backbone_blocks_1_15_bias, tu1.tensor_496);
	node_Relu_26( tu1.tensor_496, tu2.tensor_497);
	node_Cast_27( tu0.tensor_485, tu1.tensor_498);
	node_Conv_29( tu1.tensor_498, tensor_510, tensor_509, tu0.tensor_502);
	node_Relu_30( tu0.tensor_502, tu1.tensor_503);
	node_ConvTranspose_31( tu2.tensor_497, tensor_511, tu0.tensor_505);
	node_BatchNormalization_32( tu0.tensor_505, tensor_parent_decoder_neck_deblocks_1_1_weight, tensor_parent_decoder_neck_deblocks_1_1_bias, tensor_parent_decoder_neck_deblocks_1_1_running_mean, tensor_parent_decoder_neck_deblocks_1_1_running_var, tu3.tensor_506);
	node_Relu_33( tu3.tensor_506, tu0.tensor_507);
	node_Concat_34( tu1.tensor_503, tu0.tensor_507, tensor_middle);
}

static float tensor_parent_fuser_0_weight[256][336][3][3];
auto temp_parent_fuser_0_weight = read4DTensorFromFile<256,336,3,3>("../parent.fuser.0.weight.txt");
for (size_t i = 0; i < 256; ++i) {
    for (size_t j = 0; j < 336; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_fuser_0_weight[i][j][k][l] = temp_parent_fuser_0_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_fuser_0_bias[256];
auto temp_parent_fuser_0_bias = read1DTensorFromFile<256>("../parent.fuser.0.bias.txt");
for (size_t i = 0; i < 256; ++i) {
    tensor_parent_fuser_0_bias[i] = temp_parent_fuser_0_bias[i];
}
static float tensor_parent_decoder_backbone_blocks_0_0_weight[128][256][3][3];
auto temp_parent_decoder_backbone_blocks_0_0_weight = read4DTensorFromFile<128,256,3,3>("../parent.decoder.backbone.blocks.0.0.weight.txt");
for (size_t i = 0; i < 128; ++i) {
    for (size_t j = 0; j < 256; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_decoder_backbone_blocks_0_0_weight[i][j][k][l] = temp_parent_decoder_backbone_blocks_0_0_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_decoder_backbone_blocks_0_0_bias[128];
auto temp_parent_decoder_backbone_blocks_0_0_bias = read1DTensorFromFile<128>("../parent.decoder.backbone.blocks.0.0.bias.txt");
for (size_t i = 0; i < 128; ++i) {
    tensor_parent_decoder_backbone_blocks_0_0_bias[i] = temp_parent_decoder_backbone_blocks_0_0_bias[i];
}
static float tensor_parent_decoder_backbone_blocks_0_3_weight[128][128][3][3];
auto temp_parent_decoder_backbone_blocks_0_3_weight = read4DTensorFromFile<128,128,3,3>("../parent.decoder.backbone.blocks.0.3.weight.txt");
for (size_t i = 0; i < 128; ++i) {
    for (size_t j = 0; j < 128; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_decoder_backbone_blocks_0_3_weight[i][j][k][l] = temp_parent_decoder_backbone_blocks_0_3_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_decoder_backbone_blocks_0_3_bias[128];
auto temp_parent_decoder_backbone_blocks_0_3_bias = read1DTensorFromFile<128>("../parent.decoder.backbone.blocks.0.3.bias.txt");
for (size_t i = 0; i < 128; ++i) {
    tensor_parent_decoder_backbone_blocks_0_3_bias[i] = temp_parent_decoder_backbone_blocks_0_3_bias[i];
}
static float tensor_parent_decoder_backbone_blocks_0_6_weight[128][128][3][3];
auto temp_parent_decoder_backbone_blocks_0_6_weight = read4DTensorFromFile<128,128,3,3>("../parent.decoder.backbone.blocks.0.6.weight.txt");
for (size_t i = 0; i < 128; ++i) {
    for (size_t j = 0; j < 128; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_decoder_backbone_blocks_0_6_weight[i][j][k][l] = temp_parent_decoder_backbone_blocks_0_6_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_decoder_backbone_blocks_0_6_bias[128];
auto temp_parent_decoder_backbone_blocks_0_6_bias = read1DTensorFromFile<128>("../parent.decoder.backbone.blocks.0.6.bias.txt");
for (size_t i = 0; i < 128; ++i) {
    tensor_parent_decoder_backbone_blocks_0_6_bias[i] = temp_parent_decoder_backbone_blocks_0_6_bias[i];
}
static float tensor_parent_decoder_backbone_blocks_0_9_weight[128][128][3][3];
auto temp_parent_decoder_backbone_blocks_0_9_weight = read4DTensorFromFile<128,128,3,3>("../parent.decoder.backbone.blocks.0.9.weight.txt");
for (size_t i = 0; i < 128; ++i) {
    for (size_t j = 0; j < 128; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_decoder_backbone_blocks_0_9_weight[i][j][k][l] = temp_parent_decoder_backbone_blocks_0_9_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_decoder_backbone_blocks_0_9_bias[128];
auto temp_parent_decoder_backbone_blocks_0_9_bias = read1DTensorFromFile<128>("../parent.decoder.backbone.blocks.0.9.bias.txt");
for (size_t i = 0; i < 128; ++i) {
    tensor_parent_decoder_backbone_blocks_0_9_bias[i] = temp_parent_decoder_backbone_blocks_0_9_bias[i];
}
static float tensor_parent_decoder_backbone_blocks_0_12_weight[128][128][3][3];
auto temp_parent_decoder_backbone_blocks_0_12_weight = read4DTensorFromFile<128,128,3,3>("../parent.decoder.backbone.blocks.0.12.weight.txt");
for (size_t i = 0; i < 128; ++i) {
    for (size_t j = 0; j < 128; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_decoder_backbone_blocks_0_12_weight[i][j][k][l] = temp_parent_decoder_backbone_blocks_0_12_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_decoder_backbone_blocks_0_12_bias[128];
auto temp_parent_decoder_backbone_blocks_0_12_bias = read1DTensorFromFile<128>("../parent.decoder.backbone.blocks.0.12.bias.txt");
for (size_t i = 0; i < 128; ++i) {
    tensor_parent_decoder_backbone_blocks_0_12_bias[i] = temp_parent_decoder_backbone_blocks_0_12_bias[i];
}
static float tensor_parent_decoder_backbone_blocks_0_15_weight[128][128][3][3];
auto temp_parent_decoder_backbone_blocks_0_15_weight = read4DTensorFromFile<128,128,3,3>("../parent.decoder.backbone.blocks.0.15.weight.txt");
for (size_t i = 0; i < 128; ++i) {
    for (size_t j = 0; j < 128; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_decoder_backbone_blocks_0_15_weight[i][j][k][l] = temp_parent_decoder_backbone_blocks_0_15_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_decoder_backbone_blocks_0_15_bias[128];
auto temp_parent_decoder_backbone_blocks_0_15_bias = read1DTensorFromFile<128>("../parent.decoder.backbone.blocks.0.15.bias.txt");
for (size_t i = 0; i < 128; ++i) {
    tensor_parent_decoder_backbone_blocks_0_15_bias[i] = temp_parent_decoder_backbone_blocks_0_15_bias[i];
}
static float tensor_parent_decoder_backbone_blocks_1_0_weight[256][128][3][3];
auto temp_parent_decoder_backbone_blocks_1_0_weight = read4DTensorFromFile<256,128,3,3>("../parent.decoder.backbone.blocks.1.0.weight.txt");
for (size_t i = 0; i < 256; ++i) {
    for (size_t j = 0; j < 128; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_decoder_backbone_blocks_1_0_weight[i][j][k][l] = temp_parent_decoder_backbone_blocks_1_0_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_decoder_backbone_blocks_1_0_bias[256];
auto temp_parent_decoder_backbone_blocks_1_0_bias = read1DTensorFromFile<256>("../parent.decoder.backbone.blocks.1.0.bias.txt");
for (size_t i = 0; i < 256; ++i) {
    tensor_parent_decoder_backbone_blocks_1_0_bias[i] = temp_parent_decoder_backbone_blocks_1_0_bias[i];
}
static float tensor_parent_decoder_backbone_blocks_1_3_weight[256][256][3][3];
auto temp_parent_decoder_backbone_blocks_1_3_weight = read4DTensorFromFile<256,256,3,3>("../parent.decoder.backbone.blocks.1.3.weight.txt");
for (size_t i = 0; i < 256; ++i) {
    for (size_t j = 0; j < 256; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_decoder_backbone_blocks_1_3_weight[i][j][k][l] = temp_parent_decoder_backbone_blocks_1_3_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_decoder_backbone_blocks_1_3_bias[256];
auto temp_parent_decoder_backbone_blocks_1_3_bias = read1DTensorFromFile<256>("../parent.decoder.backbone.blocks.1.3.bias.txt");
for (size_t i = 0; i < 256; ++i) {
    tensor_parent_decoder_backbone_blocks_1_3_bias[i] = temp_parent_decoder_backbone_blocks_1_3_bias[i];
}
static float tensor_parent_decoder_backbone_blocks_1_6_weight[256][256][3][3];
auto temp_parent_decoder_backbone_blocks_1_6_weight = read4DTensorFromFile<256,256,3,3>("../parent.decoder.backbone.blocks.1.6.weight.txt");
for (size_t i = 0; i < 256; ++i) {
    for (size_t j = 0; j < 256; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_decoder_backbone_blocks_1_6_weight[i][j][k][l] = temp_parent_decoder_backbone_blocks_1_6_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_decoder_backbone_blocks_1_6_bias[256];
auto temp_parent_decoder_backbone_blocks_1_6_bias = read1DTensorFromFile<256>("../parent.decoder.backbone.blocks.1.6.bias.txt");
for (size_t i = 0; i < 256; ++i) {
    tensor_parent_decoder_backbone_blocks_1_6_bias[i] = temp_parent_decoder_backbone_blocks_1_6_bias[i];
}
static float tensor_parent_decoder_backbone_blocks_1_9_weight[256][256][3][3];
auto temp_parent_decoder_backbone_blocks_1_9_weight = read4DTensorFromFile<256,256,3,3>("../parent.decoder.backbone.blocks.1.9.weight.txt");
for (size_t i = 0; i < 256; ++i) {
    for (size_t j = 0; j < 256; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_decoder_backbone_blocks_1_9_weight[i][j][k][l] = temp_parent_decoder_backbone_blocks_1_9_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_decoder_backbone_blocks_1_9_bias[256];
auto temp_parent_decoder_backbone_blocks_1_9_bias = read1DTensorFromFile<256>("../parent.decoder.backbone.blocks.1.9.bias.txt");
for (size_t i = 0; i < 256; ++i) {
    tensor_parent_decoder_backbone_blocks_1_9_bias[i] = temp_parent_decoder_backbone_blocks_1_9_bias[i];
}
static float tensor_parent_decoder_backbone_blocks_1_12_weight[256][256][3][3];
auto temp_parent_decoder_backbone_blocks_1_12_weight = read4DTensorFromFile<256,256,3,3>("../parent.decoder.backbone.blocks.1.12.weight.txt");
for (size_t i = 0; i < 256; ++i) {
    for (size_t j = 0; j < 256; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_decoder_backbone_blocks_1_12_weight[i][j][k][l] = temp_parent_decoder_backbone_blocks_1_12_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_decoder_backbone_blocks_1_12_bias[256];
auto temp_parent_decoder_backbone_blocks_1_12_bias = read1DTensorFromFile<256>("../parent.decoder.backbone.blocks.1.12.bias.txt");
for (size_t i = 0; i < 256; ++i) {
    tensor_parent_decoder_backbone_blocks_1_12_bias[i] = temp_parent_decoder_backbone_blocks_1_12_bias[i];
}
static float tensor_parent_decoder_backbone_blocks_1_15_weight[256][256][3][3];
auto temp_parent_decoder_backbone_blocks_1_15_weight = read4DTensorFromFile<256,256,3,3>("../parent.decoder.backbone.blocks.1.15.weight.txt");
for (size_t i = 0; i < 256; ++i) {
    for (size_t j = 0; j < 256; ++j) {
        for (size_t k = 0; k < 3; ++k) {
            for (size_t l = 0; l < 3; ++l) {
                tensor_parent_decoder_backbone_blocks_1_15_weight[i][j][k][l] = temp_parent_decoder_backbone_blocks_1_15_weight[i][j][k][l];
            }
        }
    }
}
static float tensor_parent_decoder_backbone_blocks_1_15_bias[256];
auto temp_parent_decoder_backbone_blocks_1_15_bias = read1DTensorFromFile<256>("../parent.decoder.backbone.blocks.1.15.bias.txt");
for (size_t i = 0; i < 256; ++i) {
    tensor_parent_decoder_backbone_blocks_1_15_bias[i] = temp_parent_decoder_backbone_blocks_1_15_bias[i];
}
static float tensor_parent_decoder_neck_deblocks_1_1_weight[256];
auto temp_parent_decoder_neck_deblocks_1_1_weight = read1DTensorFromFile<256>("../parent.decoder.neck.deblocks.1.1.weight.txt");
for (size_t i = 0; i < 256; ++i) {
    tensor_parent_decoder_neck_deblocks_1_1_weight[i] = temp_parent_decoder_neck_deblocks_1_1_weight[i];
}
static float tensor_parent_decoder_neck_deblocks_1_1_bias[256];
auto temp_parent_decoder_neck_deblocks_1_1_bias = read1DTensorFromFile<256>("../parent.decoder.neck.deblocks.1.1.bias.txt");
for (size_t i = 0; i < 256; ++i) {
    tensor_parent_decoder_neck_deblocks_1_1_bias[i] = temp_parent_decoder_neck_deblocks_1_1_bias[i];
}
static float tensor_parent_decoder_neck_deblocks_1_1_running_mean[256];
auto temp_parent_decoder_neck_deblocks_1_1_running_mean = read1DTensorFromFile<256>("../parent.decoder.neck.deblocks.1.1.running_mean.txt");
for (size_t i = 0; i < 256; ++i) {
    tensor_parent_decoder_neck_deblocks_1_1_running_mean[i] = temp_parent_decoder_neck_deblocks_1_1_running_mean[i];
}
static float tensor_parent_decoder_neck_deblocks_1_1_running_var[256];
auto temp_parent_decoder_neck_deblocks_1_1_running_var = read1DTensorFromFile<256>("../parent.decoder.neck.deblocks.1.1.running_var.txt");
for (size_t i = 0; i < 256; ++i) {
    tensor_parent_decoder_neck_deblocks_1_1_running_var[i] = temp_parent_decoder_neck_deblocks_1_1_running_var[i];
}
static float tensor_509[256];
auto temp_509 = read1DTensorFromFile<256>("../509.txt");
for (size_t i = 0; i < 256; ++i) {
    tensor_509[i] = temp_509[i];
}
static float tensor_510[256][128][1][1];
auto temp_510 = read4DTensorFromFile<256,128,1,1>("../510.txt");
for (size_t i = 0; i < 256; ++i) {
    for (size_t j = 0; j < 128; ++j) {
        for (size_t k = 0; k < 1; ++k) {
            for (size_t l = 0; l < 1; ++l) {
                tensor_510[i][j][k][l] = temp_510[i][j][k][l];
            }
        }
    }
}
static float tensor_511[256][256][2][2];
auto temp_511 = read4DTensorFromFile<256,256,2,2>("../511.txt");
for (size_t i = 0; i < 256; ++i) {
    for (size_t j = 0; j < 256; ++j) {
        for (size_t k = 0; k < 2; ++k) {
            for (size_t l = 0; l < 2; ++l) {
                tensor_511[i][j][k][l] = temp_511[i][j][k][l];
            }
        }
    }
}
union tensor_union_0 {
float tensor_471[1][336][180][180];
float tensor_473[1][256][180][180];
float tensor_475[1][128][180][180];
float tensor_477[1][128][180][180];
float tensor_479[1][128][180][180];
float tensor_481[1][128][180][180];
float tensor_483[1][128][180][180];
float tensor_485[1][128][180][180];
float tensor_502[1][256][180][180];
float tensor_505[1][256][180][180];
float tensor_507[1][256][180][180];
};
static union tensor_union_0 tu0;

union tensor_union_1 {
float tensor_472[1][256][180][180];
float tensor_474[1][128][180][180];
float tensor_476[1][128][180][180];
float tensor_478[1][128][180][180];
float tensor_480[1][128][180][180];
float tensor_482[1][128][180][180];
float tensor_484[1][128][180][180];
float tensor_486[1][256][90][90];
float tensor_488[1][256][90][90];
float tensor_490[1][256][90][90];
float tensor_492[1][256][90][90];
float tensor_494[1][256][90][90];
float tensor_496[1][256][90][90];
float tensor_498[1][128][180][180];
float tensor_503[1][256][180][180];
};
static union tensor_union_1 tu1;

union tensor_union_2 {
float tensor_487[1][256][90][90];
float tensor_489[1][256][90][90];
float tensor_491[1][256][90][90];
float tensor_493[1][256][90][90];
float tensor_495[1][256][90][90];
float tensor_497[1][256][90][90];
};
static union tensor_union_2 tu2;

union tensor_union_3 {
float tensor_506[1][256][180][180];
};
static union tensor_union_3 tu3;

